{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Bidirectional, Dropout, Activation, BatchNormalization,\\\n",
    "GlobalMaxPooling1D, Reshape, TextVectorization, Embedding, GRU, SimpleRNN, Concatenate, Dot, Lambda, RepeatVector, Softmax, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 as PretrainedModel, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every image has 5 captions\n",
    "!head -n 11 captions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "captions = []\n",
    "\n",
    "with open(\"captions.txt\", 'r') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        image_caption = line.strip().split(\",\", 1)\n",
    "        if len(image_caption) == 2:\n",
    "            i, c = image_caption\n",
    "            if not images or images[-1] != i:\n",
    "                images.append(i)\n",
    "            captions.append(c)\n",
    "\n",
    "rand = np.random.randint(0, len(images))\n",
    "sample_image = images[rand]\n",
    "print(captions[rand*5:rand*5+5])\n",
    "plt.imshow(image.load_img(f'Images/{sample_image}'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images), len(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [200, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm = PretrainedModel(\n",
    "    input_shape = IMAGE_SIZE + [3],\n",
    "    weights = 'imagenet',\n",
    "    include_top = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output shape is 6*6*512\n",
    "# should reshape for LSTM to 36*512\n",
    "x = Reshape((-1, 512))(ptm.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model object\n",
    "model = Model(inputs=ptm.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = []\n",
    "\n",
    "for image_id in images:\n",
    "  image = load_img(f\"Images/{image_id}\", target_size=IMAGE_SIZE)\n",
    "  image = img_to_array(image) / 255.0\n",
    "  image = np.expand_dims(image, axis=0)\n",
    "\n",
    "  features = model.predict(image)\n",
    "  image_features.append(features.squeeze(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_features[rand].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"<start>\"\n",
    "end_token = \"<end>\"\n",
    "\n",
    "for i in range(len(captions)):\n",
    "    captions[i] = captions[i].lower()\n",
    "    captions[i] = re.sub(r\"[^a-z\\s]\", \"\", captions[i])\n",
    "    captions[i] = f\"{start_token} {captions[i]} {end_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(captions[rand*5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_layer = TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE\n",
    ")\n",
    "vectorization_layer.adapt(captions)\n",
    "vectorized_captions = vectorization_layer(captions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for c in vectorized_captions:\n",
    "    if len(c) != 37:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorized_captions[rand*5])\n",
    "print(len(vectorized_captions[rand*5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_features, vectorized_captions, batch_size):\n",
    "    while True:\n",
    "        X_image_batch = []\n",
    "        X_caption_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for i, features in enumerate(image_features):\n",
    "            cap_of_img = vectorized_captions[i*5:i*5+5]\n",
    "\n",
    "            for cap in cap_of_img:\n",
    "                for t in range(1, len(cap)):\n",
    "                    input_caption = cap[:t]\n",
    "                    target = cap[t]\n",
    "\n",
    "                    X_image_batch.append(features)\n",
    "                    X_caption_batch.append(input_caption)\n",
    "                    y_batch.append(target)\n",
    "\n",
    "                    if len(X_image_batch) == batch_size:\n",
    "                        X_caption_batch = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                            X_caption_batch, maxlen=37, padding='post'\n",
    "                        )\n",
    "                        yield {\n",
    "                            'image_input': np.array(X_image_batch),\n",
    "                            'caption_input': np.array(X_caption_batch)\n",
    "                        }, np.array(y_batch)\n",
    "                        X_image_batch, X_caption_batch, y_batch = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition with explicit layer names\n",
    "img_i = Input(shape=(36, 512), name='image_input')\n",
    "cap_i = Input(shape=(37,), name='caption_input', dtype='int32')\n",
    "\n",
    "# encoder\n",
    "img_x = GlobalMaxPooling1D()(img_i) # (N, 36, 512) -> (N, 512)\n",
    "img_x = Dense(256, activation='relu')(img_x) # (N, 512) -> (N, 256)\n",
    "img_x = BatchNormalization()(img_x) # (N, 256) -> (N, 256)\n",
    "img_x = Dropout(0.2)(img_x) # (N, 256) -> (N, 256)\n",
    "\n",
    "# embedding (masking tells model to ignore the padding)\n",
    "cap_x = Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=256, mask_zero=True)(cap_i) # (N, 37) -> (N, 37, 256)\n",
    "cap_x = Dropout(0.2)(cap_x) # (N, 37, 256) -> (N, 37, 256)\n",
    "cap_x = Bidirectional(LSTM(256, return_sequences=True))(cap_x) # (N, 37, 256) -> (N, 37, 512)\n",
    "cap_x = BatchNormalization()(cap_x) # (N, 37, 512) -> (N, 37, 512)\n",
    "cap_x = Dropout(0.2)(cap_x) # (N, 37, 512) -> (N, 37, 512)\n",
    "\n",
    "# attention\n",
    "attention_x = Dense(128, activation='tanh')(cap_x) # (N, 37, 512) -> (N, 37, 128)\n",
    "attention_x = BatchNormalization()(attention_x) # (N, 37, 128) -> (N, 37, 128)\n",
    "attention_scores = Dense(1, activation='tanh')(attention_x) # (N, 37, 128) -> (N, 37, 1)\n",
    "attention_scores = Reshape((37,))(attention_scores) # (N, 37, 1) -> (N, 37)\n",
    "attention_weights = Dense(1, activation='softmax')(attention_scores) # (N, 37) -> (N, 1)\n",
    "weighted_sum = Multiply()([cap_x, attention_weights]) # (N, 37, 512) * (N, 1) -> (N, 37, 512)\n",
    "cap_x = GlobalMaxPooling1D()(weighted_sum) # (N, 37, 512) -> (N, 512)\n",
    "\n",
    "# combine\n",
    "x = Concatenate()([img_x, cap_x]) # (N, 256) and (N, 512) -> (N, 768)\n",
    "x = Dense(512, activation='relu')(x) # (N, 768) -> (N, 512)\n",
    "x = BatchNormalization()(x) # (N, 512) -> (N, 512)\n",
    "x = Dropout(0.2)(x) # (N, 512) -> (N, 512)\n",
    "x = Dense(MAX_VOCAB_SIZE, activation='softmax')(x) # (N, 512) -> (N, MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[img_i, cap_i], outputs=x)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = len(image_features) * 4 // 5\n",
    "\n",
    "train_features = image_features[:split_index]\n",
    "val_features = image_features[split_index:]\n",
    "\n",
    "train_captions = vectorized_captions[:split_index * 5]\n",
    "val_captions = vectorized_captions[split_index * 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "steps_per_epoch = len(train_features) * 5 // batch_size\n",
    "validation_steps = len(val_features) * 5 // batch_size\n",
    "\n",
    "# Create TensorFlow datasets with matching input names\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_features, train_captions, batch_size),\n",
    "    output_signature=(\n",
    "        {\n",
    "            'image_input': tf.TensorSpec(shape=(batch_size, 36, 512), dtype=tf.float32),\n",
    "            'caption_input': tf.TensorSpec(shape=(batch_size, 37), dtype=tf.float32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_features, val_captions, batch_size),\n",
    "    output_signature=(\n",
    "        {\n",
    "            'image_input': tf.TensorSpec(shape=(batch_size, 36, 512), dtype=tf.float32),\n",
    "            'caption_input': tf.TensorSpec(shape=(batch_size, 37), dtype=tf.float32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "r = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"captioning_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_features, vectorization_layer, max_length=37):\n",
    "    # Initialize with start token\n",
    "    caption = ['<start>']\n",
    "    \n",
    "    # Convert to sequence\n",
    "    input_caption = vectorization_layer(caption).numpy()\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for i in range(max_length):\n",
    "        # Prepare inputs\n",
    "        current_caption = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [input_caption], maxlen=37, padding='post'\n",
    "        )\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = model.predict(\n",
    "            {\n",
    "                'image_input': np.expand_dims(image_features, axis=0),\n",
    "                'caption_input': current_caption\n",
    "            }, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get predicted word index\n",
    "        predicted_id = np.argmax(predictions[0])\n",
    "        \n",
    "        # Convert to word\n",
    "        predicted_word = vectorization_layer.get_vocabulary()[predicted_id]\n",
    "        \n",
    "        # Break if end token\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "            \n",
    "        # Add to caption\n",
    "        caption.append(predicted_word)\n",
    "        input_caption = vectorization_layer(caption).numpy()\n",
    "    \n",
    "    return ' '.join(caption[1:])  # Remove start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(model, val_features, val_captions, vectorization_layer):\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # Generate predictions for validation set\n",
    "    for i, image_feature in enumerate(val_features):\n",
    "        # Get reference captions for this image (5 captions per image)\n",
    "        image_captions = val_captions[i*5:(i+1)*5]\n",
    "        reference = []\n",
    "        \n",
    "        # Process reference captions\n",
    "        for cap in image_captions:\n",
    "            # Convert indices back to words and tokenize\n",
    "            cap_words = [vectorization_layer.get_vocabulary()[idx] for idx in cap if idx != 0]\n",
    "            # Remove start and end tokens\n",
    "            cap_words = [word for word in cap_words if word not in ['<start>', '<end>', '']]\n",
    "            reference.append(cap_words)\n",
    "        \n",
    "        references.append(reference)\n",
    "        \n",
    "        # Generate prediction\n",
    "        predicted_caption = generate_caption(model, image_feature, vectorization_layer)\n",
    "        # Tokenize prediction\n",
    "        hypothesis = predicted_caption.split()\n",
    "        # Remove start and end tokens if present\n",
    "        hypothesis = [word for word in hypothesis if word not in ['<start>', '<end>', '']]\n",
    "        \n",
    "        hypotheses.append(hypothesis)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{len(val_features)} images\")\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        'bleu1': bleu1,\n",
    "        'bleu2': bleu2,\n",
    "        'bleu3': bleu3,\n",
    "        'bleu4': bleu4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = evaluate_bleu(model, val_features, val_captions, vectorization_layer)\n",
    "\n",
    "print(\"BLEU Scores:\")\n",
    "for metric, score in bleu_scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
